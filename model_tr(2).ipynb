{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#インポート\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import unicodedata\n",
    "import itertools\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_version=\"2023-12-01-preview\"\n",
    "engine = 'gpt-4o-mini'\n",
    "gpt = AzureOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY_GPT4OMINI\"),  \n",
    "    api_version=api_version,\n",
    "    azure_endpoint = os.getenv(\"OPENAI_API_BASE_GPT4OMINI\")\n",
    "    )\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "plamo = OpenAI(\n",
    "    base_url=\"https://platform.preferredai.jp/api/completion/v1\",\n",
    "    api_key='NjcyYWFhNjUyZWZlMzAwZjJmMzEyN2FlOkRyeDB3enhyQWJsSm9rcENMdFR0WDhYanRrMDc2RnEy'\n",
    "    # other params...,\n",
    ")\n",
    "\n",
    "\n",
    "data = pd.read_csv('../data.csv')\n",
    "import json\n",
    "import enum\n",
    "from typing_extensions import TypedDict\n",
    "import typing_extensions as typing\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_content():\n",
    "    def __init__(self, path):\n",
    "        # 各doc_idのパスを指定\n",
    "        self.path = path\n",
    "        self.doc_id = path.split('/')[-1]\n",
    "    \n",
    "    def edit_text(self, text):\n",
    "        return re.sub(r'[^\\u3040-\\u309F\\u30A0-\\u30FF\\uFF65-\\uFF9F\\u4E00-\\u9FFF]', '', text)\n",
    "    \n",
    "    def make_content(self):\n",
    "        try:\n",
    "            with open(f\"../data/contents/{self.doc_id}.pkl\", 'rb') as f:\n",
    "                dct = pickle.load(f)\n",
    "                f.close()\n",
    "            return dct\n",
    "        except:\n",
    "            dct = pd.DataFrame()\n",
    "            files = os.listdir(self.path)\n",
    "            tmp_h1 = ''\n",
    "            tmp_h2 = ''\n",
    "            tmp_h3 = ''\n",
    "            tmp_h4 = ''\n",
    "            tmp_h5 = ''\n",
    "            tmp_h6 = ''\n",
    "            if not os.path.exists(f'../data/{self.doc_id}'):\n",
    "                os.makedirs(f'../data/{self.doc_id}')\n",
    "            for file in files:\n",
    "                with open(f'{self.path}/{file}', 'r') as f:\n",
    "                    soup = bs(f, 'html.parser')\n",
    "                    f.close()\n",
    "                html_tag = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'table'])\n",
    "                for index, tag in enumerate(html_tag):\n",
    "                    if tag.name == 'h1':\n",
    "                        text = unicodedata.normalize('NFKC', tag.get_text()).replace('\\n', '').replace(' ', '')\n",
    "                        if text == '':\n",
    "                            continue\n",
    "                        tmp_h1 = self.edit_text(text)\n",
    "                        tmp_h2 = ''\n",
    "                        tmp_h3 = ''\n",
    "                        tmp_h4 = ''\n",
    "                        tmp_h5 = ''\n",
    "                        tmp_h6 = ''\n",
    "                    elif tag.name == 'h2':\n",
    "                        text = unicodedata.normalize('NFKC', tag.get_text()).replace('\\n', '').replace(' ', '')\n",
    "                        if text == '':\n",
    "                            continue\n",
    "                        tmp_h2 = self.edit_text(text)\n",
    "                        tmp_h3 = ''\n",
    "                        tmp_h4 = ''\n",
    "                        tmp_h5 = ''\n",
    "                        tmp_h6 = ''\n",
    "                    elif tag.name == 'h3':\n",
    "                        text = unicodedata.normalize('NFKC', tag.get_text()).replace('\\n', '').replace(' ', '')\n",
    "                        if text == '':\n",
    "                            continue\n",
    "                        tmp_h3 = self.edit_text(text)\n",
    "                        tmp_h4 = ''\n",
    "                        tmp_h5 = ''\n",
    "                        tmp_h6 = ''\n",
    "                    elif tag.name == 'h4':\n",
    "                        text = unicodedata.normalize('NFKC', tag.get_text()).replace('\\n', '').replace(' ', '')\n",
    "                        if text == '':\n",
    "                            continue\n",
    "                        tmp_h4 = self.edit_text(text)\n",
    "                        tmp_h5 = ''\n",
    "                        tmp_h6 = ''\n",
    "                    elif tag.name == 'h5':\n",
    "                        text = unicodedata.normalize('NFKC', tag.get_text()).replace('\\n', '').replace(' ', '')\n",
    "                        if text == '':\n",
    "                            continue\n",
    "                        tmp_h5 = self.edit_text(text)\n",
    "                        tmp_h6 = ''\n",
    "                    elif tag.name == 'h6':\n",
    "                        text = unicodedata.normalize('NFKC', tag.get_text()).replace('\\n', '').replace(' ', '')\n",
    "                        if text == '':\n",
    "                            continue\n",
    "                        tmp_h6 = self.edit_text(text)\n",
    "                    elif tag.name == 'table':\n",
    "                        dct =pd.concat([dct, pd.DataFrame({'h1': [tmp_h1], 'h2': [tmp_h2], 'h3': [tmp_h3], 'h4': [tmp_h4], 'h5': [tmp_h5], 'h6': [tmp_h6], 'table_id':[tag.get('table-id')],'html': [file]})])\n",
    "            pickle.dump(dct, open(f\"../data/contents/{self.doc_id}.pkl\", 'wb'))\n",
    "            return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for _, rows in data.iterrows():\n",
    "    path = f'../{rows[\"flag\"]}/reports/{rows[\"doc_id\"]}'\n",
    "    make_content(path).make_content()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class search_table():\n",
    "    def __init__(self, words, doc_id, df_train, question):\n",
    "        self.words = words\n",
    "        self.doc_id = doc_id\n",
    "        self.df_train = df_train\n",
    "        self.question = question\n",
    "\n",
    "    # 類似質問から目次データを取得\n",
    "    def get_contents(self, ):\n",
    "        #重複単語数でランク付けしたほうがいいかもしれない\n",
    "        f = lambda x:sum([(word in x) for word in self.words]) \n",
    "        similary_tables = self.df_train[self.df_train['words'].apply(f) == self.df_train['words'].apply(f).max()]['table_id'].values\n",
    "        \n",
    "        df_contents = pd.DataFrame()\n",
    "        for table_id in similary_tables:\n",
    "            doc_id = table_id.split('-')[0]\n",
    "            with open(f\"../data/contents/{doc_id}.pkl\", 'rb') as f:\n",
    "                dct = pickle.load(f)\n",
    "                f.close()\n",
    "            df_contents = pd.concat([df_contents, dct.query('table_id == @table_id')], axis=0)\n",
    "\n",
    "        return df_contents.drop_duplicates(subset=['h1', 'h2', 'h3', 'h4', 'h5'])\n",
    "    \n",
    "    #類似質問の目次データを該当文書のtable_id候補を取得する\n",
    "    def get_tables(self,):\n",
    "        contents = self.get_contents()\n",
    "        contents.reset_index(inplace=True)\n",
    "        with open(f\"../data/contents/{self.doc_id}.pkl\", 'rb') as f:\n",
    "            myself_contents = pickle.load(f)\n",
    "            f.close()\n",
    "            \n",
    "        if contents.empty:\n",
    "            return myself_contents['table_id'].values\n",
    "    \n",
    "        tables_dict= {}\n",
    "        for index, rows in contents.iterrows():\n",
    "            tmp = myself_contents\n",
    "            tmp_header = 'h0'\n",
    "            if rows['h1'] != '' and not myself_contents.query('h1 == @rows[\"h1\"]').empty:\n",
    "                tmp = myself_contents.query('h1 == @rows[\"h1\"]',)\n",
    "                tmp_header = 'h1'\n",
    "                \n",
    "            if rows['h2'] != '' and not myself_contents.query('h2 == @rows[\"h2\"]').empty:\n",
    "                tmp = myself_contents.query('h2 == @rows[\"h2\"]',)\n",
    "                tmp_header = 'h2'\n",
    "\n",
    "            if rows['h3'] != '' and not myself_contents.query('h3 == @rows[\"h3\"] or h4 == @rows[\"h3\"]').empty:\n",
    "                tmp = myself_contents.query('h3 == @rows[\"h3\"] or h4 == @rows[\"h3\"]')\n",
    "                tmp_header = 'h3'\n",
    "            \n",
    "            if rows['h4'] != '' and not myself_contents.query('h4 == @rows[\"h4\"] or h5 == @rows[\"h4\"]').empty:\n",
    "                tmp = myself_contents.query('h4 == @rows[\"h4\"] or h5 == @rows[\"h4\"]')\n",
    "                tmp_header = 'h4'\n",
    "\n",
    "            if rows['h5'] != '' and not myself_contents.query('h5 == @rows[\"h5\"] or h4 == @rows[\"h5\"]').empty:\n",
    "                tmp = myself_contents.query('h5 == @rows[\"h5\"] or h4 == @rows[\"h5\"]',)\n",
    "                tmp_header = 'h5'\n",
    "            \n",
    "            tables_dict[f'no{index}'] = {'depth':tmp_header, 'tables':tmp['table_id'].values}\n",
    "    \n",
    "        max_depth = max(d['depth'] for d in tables_dict.values())\n",
    "        tables = [d['tables'] for d in tables_dict.values() if d['depth'] == max_depth]\n",
    "        tables = list(set(itertools.chain.from_iterable(tables)))\n",
    "        return tables\n",
    "\n",
    "    def answer_table(self):\n",
    "        targets_tables = self.get_tables()\n",
    "        if len(targets_tables) == 1:\n",
    "            return targets_tables \n",
    "        else:\n",
    "            tables_score = {}\n",
    "            text = ''\n",
    "            for table in targets_tables:\n",
    "                html = table.split('-')[1]\n",
    "                if not os.path.exists(f'../data/{self.doc_id}'):\n",
    "                    os.mkdir(f'../data/{self.doc_id}')\n",
    "                \n",
    "                if os.path.exists(f'../data/{self.doc_id}/{table}.pickle'):                   \n",
    "                    with open(f'../data/{self.doc_id}/{table}.pickle', 'r',) as f:\n",
    "                        text = f.read()\n",
    "                        f.close()\n",
    "                else:\n",
    "                    with open(f\"../test/reports/{self.doc_id}/{html}.html\", 'rb') as f:\n",
    "                        soup = bs(f, 'html.parser')\n",
    "                        f.close()\n",
    "                    tags = soup.find_all(['table', 'p', 'h3', 'h4', 'h5'])\n",
    "                    p_text = ''\n",
    "                    text = ''\n",
    "                    tmp_h3 = ''\n",
    "                    tmp_h4 = ''\n",
    "                    tmp_h5 = ''\n",
    "                    for tag in tags:\n",
    "                        if tag.name == 'h3':\n",
    "                            tmp_h3 = unicodedata.normalize('NFKC', tag.get_text()).replace('\\n', '')\n",
    "                            tmp_h4 = ''\n",
    "                            tmp_h5 = ''\n",
    "                        if tag.name == 'h4':\n",
    "                            tmp_h4 = unicodedata.normalize('NFKC', tag.get_text()).replace('\\n', '')\n",
    "                            tmp_h5 = ''\n",
    "                        if tag.name == 'h5':\n",
    "                            tmp_h5 = unicodedata.normalize('NFKC', tag.get_text()).replace('\\n', '')\n",
    "                        if tag.name == 'table':\n",
    "                            if tag.get('table-id') == table:\n",
    "                                tab_text = unicodedata.normalize(\"NFKC\", tag.get_text()).replace(\"\\n\", \"\")\n",
    "                                text = f'{tmp_h3}\\n{tmp_h4}\\n{tmp_h5}\\n{tab_text}\\n{p_text}'\n",
    "                                break\n",
    "                            else:\n",
    "                                p_text = ''\n",
    "                        else:\n",
    "                            if tag.parent.name != 'td':\n",
    "                                p_text = unicodedata.normalize('NFKC', tag.get_text()).replace('\\n', '').replace(' ', '')\n",
    "                    with open(f'../data/{self.doc_id}/{table}.pickle', 'w', encoding='utf-8') as f2:\n",
    "                        f2.write(text)\n",
    "                        f2.close()\n",
    "                tables_score[table] = sum([(word in text) for word in self.words])\n",
    "            return [kv[0] for kv in tables_score.items() if kv[1] == max(tables_score.values())]\n",
    "\n",
    "    def edit_question(self, ):\n",
    "        year = re.search('20(\\d+)', self.question).group()\n",
    "        if year == '2024':\n",
    "            question=re.sub(f'{year}年', '最新', self.question)\n",
    "        elif year == '2023':\n",
    "            question=re.sub(f'{year}年', '前年', self.question)\n",
    "        elif year == '2022':\n",
    "            question=re.sub(f'{year}年', '前々年', self.question)\n",
    "        elif year == '2021':\n",
    "            question=re.sub(f'{year}年', '前前々年', self.question)\n",
    "        elif year == '2020':\n",
    "            question=re.sub(f'{year}年', '前前前々年', self.question)\n",
    "\n",
    "        if re.search('「(.*)」', self.question):\n",
    "            words = re.search('「(.*)」', self.question).group()[1:-1].split('、')[::-1]\n",
    "            question = re.sub('「(.*)」', 'の'.join(words), question)\n",
    "\n",
    "        if re.search('[a-zA-Z]+', self.question): \n",
    "            prompt =f\"次単語を日本語に変換して答えのみを出力してください。人名も漢字をあててください。\\n{re.search('[a-zA-Z]+', self.question).group()}\"\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"あなたは翻訳家です。\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "\n",
    "            response = plamo.chat.completions.create(\n",
    "                model=\"plamo-beta\",\n",
    "                #model=engine, \n",
    "                messages = messages,\n",
    "            )\n",
    "            question = re.sub(re.search('[a-zA-Z]+', self.question).group(), response.choices[0].message.content, question)\n",
    "        return question\n",
    "    \n",
    "    #ChatGPTに質問を投げて、表から回答を抽出する\n",
    "    def answer(self, ):\n",
    "        tables = self.answer_table()\n",
    "        if len(tables) == 1:\n",
    "            return tables[0]\n",
    "        elif len(tables) > 5:\n",
    "            return tables[0]\n",
    "        else:\n",
    "            table_text = ''\n",
    "            for i, table in enumerate(tables):\n",
    "                if os.path.exists(f'../data/{self.doc_id}/{table}.pickle'):                   \n",
    "                    with open(f'../data/{self.doc_id}/{table}.pickle', 'r',) as f:\n",
    "                        table_text +=f\"\"\"##Table{i}\\n{f.read()}\\n\"\"\"\n",
    "                        f.close()\n",
    "        \n",
    "            prompt =f\"質問に回答が記載されている表の番号を教えてください。特に明記されていない場合は提出会社の決算を答えてください。\\n##question\\n{self.edit_question()}\\n{table_text}\\n##出力結果\\nTable_num:<Table_num>\"\n",
    "            \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"あなたは情報抽出タスクを行うタスクを任されてました。\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "\n",
    "            response = gpt.chat.completions.create(\n",
    "                #model=\"plamo-beta\",\n",
    "                model=engine,\n",
    "                messages = messages,\n",
    "            )\n",
    "            try:\n",
    "                return tables[int(re.search('\\d', response.choices[0].message.content).group())]\n",
    "            except:\n",
    "                return table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本光電工業株式会社の2020年における「当期純利益又は当期純損失（△）、経営指標等」は？\n",
      "S100TUGR-0101010-tab3\n",
      "2\n",
      "Table_num:1\n"
     ]
    }
   ],
   "source": [
    "df_train = data.query('flag in [\"train\",]')\n",
    "question, words, doc_id, table_id = data.query('question_number ==\"question_tr_valid2\"')[['question',\"words\",\"doc_id\", 'table_id']].values[0]\n",
    "words = json.loads(words.replace(\"'\", '\"'))\n",
    "print(question)\n",
    "print(table_id)\n",
    "print(search_table(words, doc_id, df_train, question).answer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:07,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:08,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:10,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:13,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:17,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:18,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:20,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:22,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:25,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:27,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:29,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:34,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:38,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:40,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:42,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:49,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:50,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:53,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:55,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:57,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [01:03,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [01:07,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [01:09,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [01:15,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [01:16,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [01:18,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [03:50, 47.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [03:52, 33.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [03:54, 24.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [03:57, 17.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [04:01, 13.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [04:04, 10.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [04:06,  8.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [04:13,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [04:21,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [04:25,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [04:29,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [04:30,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [04:40,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [04:46,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [04:51,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [04:54,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [04:55,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [04:59,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [05:07,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [05:12,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_test392\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "df_train = data.query('flag in [\"train\", \"valid\"]')\n",
    "answer_sheet = {}\n",
    "for index, row in tqdm(data.query('flag ==\"test\"')[344:].iterrows()):\n",
    "    print(row['question_number'])\n",
    "    words, doc_id, table_id = row[[\"words\",\"doc_id\", 'table_id']]\n",
    "    words = json.loads(words.replace(\"'\", '\"'))\n",
    "    answer = search_table(words, doc_id, df_train, question).answer()\n",
    "    answer_sheet[row['question_number']] = answer\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_tr_valid7, TRUE:S100TL6G-0105100-tab162, PRED:S100TL6G-0105050-tab129\n",
      "question_tr_valid8, TRUE:S100T4ZN-0104010-tab92, PRED:S100T4ZN-0104010-tab91\n",
      "question_tr_valid14, TRUE:S100TLL3-0105100-tab81, PRED:S100TLL3-0105010-tab70\n",
      "question_tr_valid17, TRUE:S100TVWH-0105010-tab137, PRED:S100TVWH-0105010-tab118\n",
      "question_tr_valid19, TRUE:S100TTWS-0105010-tab163, PRED:S100TTWS-0105010-tab164\n",
      "question_tr_valid20, TRUE:S100TPPL-0104010-tab112, PRED:S100TPPL-0104010-tab116\n",
      "question_tr_valid44, TRUE:S100TPPL-0105330-tab266, PRED:S100TPPL-0105330-tab267\n",
      "question_tr_valid47, TRUE:S100TNH4-0105040-tab83, PRED:S100TNH4-0105040-tab85\n",
      "0.84\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "gold_sd = dict(data.query('flag == \"valid\"')[['question_number', 'table_id']].values)\n",
    "for key in answer_sheet.keys():\n",
    "    total += 1\n",
    "    if gold_sd[key] in answer_sheet[key]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        print(f'{key}, TRUE:{gold_sd[key]}, PRED:{answer_sheet[key]}')\n",
    "        pass\n",
    "        \n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(answer_sheet,open('../test/answersheet2.json', 'w') ,indent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

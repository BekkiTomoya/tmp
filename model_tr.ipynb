{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#インポート\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import unicodedata\n",
    "import itertools\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_version=\"2023-12-01-preview\"\n",
    "engine = 'gpt-4o-mini'\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY_GPT4OMINI\"),  \n",
    "    api_version=api_version,\n",
    "    azure_endpoint = os.getenv(\"OPENAI_API_BASE_GPT4OMINI\")\n",
    "    )\n",
    "\n",
    "data = pd.read_csv('../data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_sd = dict(data.query('flag ==\"test\"')[['question_number', 'table_id']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_content():\n",
    "    def __init__(self, path):\n",
    "        # 各doc_idのパスを指定\n",
    "        self.path = path\n",
    "        self.doc_id = path.split('/')[-1]\n",
    "    \n",
    "    def edit_text(self, text):\n",
    "        return re.sub(r'[^\\u3040-\\u309F\\u30A0-\\u30FF\\uFF65-\\uFF9F\\u4E00-\\u9FFF]', '', text)\n",
    "    \n",
    "    def make_content(self):\n",
    "        try:\n",
    "            with open(f\"../data/contents/{self.doc_id}.pkl\", 'rb') as f:\n",
    "                dct = pickle.load(f)\n",
    "                f.close()\n",
    "            return dct\n",
    "        except:\n",
    "            dct = pd.DataFrame()\n",
    "            files = os.listdir(self.path)\n",
    "            tmp_h1 = ''\n",
    "            tmp_h2 = ''\n",
    "            tmp_h3 = ''\n",
    "            tmp_h4 = ''\n",
    "            tmp_h5 = ''\n",
    "            tmp_h6 = ''\n",
    "            if not os.path.exists(f'../data/{self.doc_id}'):\n",
    "                os.makedirs(f'../data/{self.doc_id}')\n",
    "            for file in files:\n",
    "                with open(f'{self.path}/{file}', 'r') as f:\n",
    "                    soup = bs(f, 'html.parser')\n",
    "                    f.close()\n",
    "                html_tag = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'table'])\n",
    "                for index, tag in enumerate(html_tag):\n",
    "                    if tag.name == 'h1':\n",
    "                        text = unicodedata.normalize('NFKC', tag.get_text()).replace('\\n', '').replace(' ', '')\n",
    "                        if text == '':\n",
    "                            continue\n",
    "                        tmp_h1 = self.edit_text(text)\n",
    "                        tmp_h2 = ''\n",
    "                        tmp_h3 = ''\n",
    "                        tmp_h4 = ''\n",
    "                        tmp_h5 = ''\n",
    "                        tmp_h6 = ''\n",
    "                    elif tag.name == 'h2':\n",
    "                        text = unicodedata.normalize('NFKC', tag.get_text()).replace('\\n', '').replace(' ', '')\n",
    "                        if text == '':\n",
    "                            continue\n",
    "                        tmp_h2 = self.edit_text(text)\n",
    "                        tmp_h3 = ''\n",
    "                        tmp_h4 = ''\n",
    "                        tmp_h5 = ''\n",
    "                        tmp_h6 = ''\n",
    "                    elif tag.name == 'h3':\n",
    "                        text = unicodedata.normalize('NFKC', tag.get_text()).replace('\\n', '').replace(' ', '')\n",
    "                        if text == '':\n",
    "                            continue\n",
    "                        tmp_h3 = self.edit_text(text)\n",
    "                        tmp_h4 = ''\n",
    "                        tmp_h5 = ''\n",
    "                        tmp_h6 = ''\n",
    "                    elif tag.name == 'h4':\n",
    "                        text = unicodedata.normalize('NFKC', tag.get_text()).replace('\\n', '').replace(' ', '')\n",
    "                        if text == '':\n",
    "                            continue\n",
    "                        tmp_h4 = self.edit_text(text)\n",
    "                        tmp_h5 = ''\n",
    "                        tmp_h6 = ''\n",
    "                    elif tag.name == 'h5':\n",
    "                        text = unicodedata.normalize('NFKC', tag.get_text()).replace('\\n', '').replace(' ', '')\n",
    "                        if text == '':\n",
    "                            continue\n",
    "                        tmp_h5 = self.edit_text(text)\n",
    "                        tmp_h6 = ''\n",
    "                    elif tag.name == 'h6':\n",
    "                        text = unicodedata.normalize('NFKC', tag.get_text()).replace('\\n', '').replace(' ', '')\n",
    "                        if text == '':\n",
    "                            continue\n",
    "                        tmp_h6 = self.edit_text(text)\n",
    "                    elif tag.name == 'table':\n",
    "                        dct =pd.concat([dct, pd.DataFrame({'h1': [tmp_h1], 'h2': [tmp_h2], 'h3': [tmp_h3], 'h4': [tmp_h4], 'h5': [tmp_h5], 'h6': [tmp_h6], 'table_id':[tag.get('table-id')],'html': [file]})])\n",
    "            pickle.dump(dct, open(f\"../data/contents/{self.doc_id}.pkl\", 'wb'))\n",
    "            return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for _, rows in data.iterrows():\n",
    "    path = f'../{rows[\"flag\"]}/reports/{rows[\"doc_id\"]}'\n",
    "    make_content(path).make_content()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class search_table():\n",
    "    def __init__(self, words, doc_id, df_train, question):\n",
    "        self.words = words\n",
    "        self.doc_id = doc_id\n",
    "        self.df_train = df_train\n",
    "        self.question = question\n",
    "\n",
    "    # 類似質問から目次データを取得\n",
    "    def get_contents(self, ):\n",
    "        #重複単語数でランク付けしたほうがいいかもしれない\n",
    "        f = lambda x:sum([(word in x) for word in self.words]) \n",
    "        similary_tables = self.df_train[self.df_train['words'].apply(f) == self.df_train['words'].apply(f).max()]['table_id'].values\n",
    "        \n",
    "        df_contents = pd.DataFrame()\n",
    "        for table_id in similary_tables:\n",
    "            doc_id = table_id.split('-')[0]\n",
    "            with open(f\"../data/contents/{doc_id}.pkl\", 'rb') as f:\n",
    "                dct = pickle.load(f)\n",
    "                f.close()\n",
    "            df_contents = pd.concat([df_contents, dct.query('table_id == @table_id')], axis=0)\n",
    "\n",
    "        return df_contents.drop_duplicates(subset=['h1', 'h2', 'h3', 'h4', 'h5'])\n",
    "    \n",
    "    #類似質問の目次データを該当文書のtable_id候補を取得する\n",
    "    def get_tables(self,):\n",
    "        contents = self.get_contents()\n",
    "        contents.reset_index(inplace=True)\n",
    "        with open(f\"../data/contents/{self.doc_id}.pkl\", 'rb') as f:\n",
    "            myself_contents = pickle.load(f)\n",
    "            f.close()\n",
    "            \n",
    "        if contents.empty:\n",
    "            return myself_contents['table_id'].values\n",
    "    \n",
    "        tables_dict= {}\n",
    "        for index, rows in contents.iterrows():\n",
    "            tmp = myself_contents\n",
    "            tmp_header = 'h0'\n",
    "            if rows['h1'] != '' and not myself_contents.query('h1 == @rows[\"h1\"]').empty:\n",
    "                tmp = myself_contents.query('h1 == @rows[\"h1\"]',)\n",
    "                tmp_header = 'h1'\n",
    "                \n",
    "            if rows['h2'] != '' and not myself_contents.query('h2 == @rows[\"h2\"]').empty:\n",
    "                tmp = myself_contents.query('h2 == @rows[\"h2\"]',)\n",
    "                tmp_header = 'h2'\n",
    "\n",
    "            if rows['h3'] != '' and not myself_contents.query('h3 == @rows[\"h3\"] or h4 == @rows[\"h3\"]').empty:\n",
    "                tmp = myself_contents.query('h3 == @rows[\"h3\"] or h4 == @rows[\"h3\"]')\n",
    "                tmp_header = 'h3'\n",
    "            \n",
    "            if rows['h4'] != '' and not myself_contents.query('h4 == @rows[\"h4\"] or h5 == @rows[\"h4\"]').empty:\n",
    "                tmp = myself_contents.query('h4 == @rows[\"h4\"] or h5 == @rows[\"h4\"]')\n",
    "                tmp_header = 'h4'\n",
    "\n",
    "            if rows['h5'] != '' and not myself_contents.query('h5 == @rows[\"h5\"] or h4 == @rows[\"h5\"]').empty:\n",
    "                tmp = myself_contents.query('h5 == @rows[\"h5\"] or h4 == @rows[\"h5\"]',)\n",
    "                tmp_header = 'h5'\n",
    "            \n",
    "            tables_dict[f'no{index}'] = {'depth':tmp_header, 'tables':tmp['table_id'].values}\n",
    "    \n",
    "        max_depth = max(d['depth'] for d in tables_dict.values())\n",
    "        tables = [d['tables'] for d in tables_dict.values() if d['depth'] == max_depth]\n",
    "        tables = list(set(itertools.chain.from_iterable(tables)))\n",
    "        return tables\n",
    "\n",
    "    def answer_table(self):\n",
    "        targets_tables = self.get_tables()\n",
    "        if len(targets_tables) == 1:\n",
    "            return targets_tables \n",
    "        else:\n",
    "            tables_score = {}\n",
    "            text = ''\n",
    "            for table in targets_tables:\n",
    "                html = table.split('-')[1]\n",
    "                if os.path.exists(f'../data/{self.doc_id}/{table}.pickle'):                   \n",
    "                    with open(f'../data/{self.doc_id}/{table}.pickle', 'r',) as f:\n",
    "                        text = f.read()\n",
    "                        f.close()\n",
    "                else:\n",
    "                    with open(f\"../test/reports/{self.doc_id}/{html}.html\", 'rb') as f:\n",
    "                        soup = bs(f, 'html.parser')\n",
    "                        f.close()\n",
    "                    tags = soup.find_all(['table', 'p', 'h3', 'h4'])\n",
    "                    for tag in tags:\n",
    "                        if tag.name == 'table':\n",
    "                            if tag.get('table-id') == table:\n",
    "                                text += unicodedata.normalize('NFKC', tag.get_text()).replace(' ', '').replace('\\n', '')\n",
    "                                break\n",
    "                            else:\n",
    "                                text = ''\n",
    "                        else:\n",
    "                            if tag.parent.name != 'td':\n",
    "                                text += unicodedata.normalize('NFKC', tag.get_text()).replace('\\n', '').replace(' ', '')\n",
    "                    with open(f'../data/{self.doc_id}/{table}.pickle', 'w', encoding='utf-8') as f2:\n",
    "                        f2.write(text)\n",
    "                        f2.close()\n",
    "                tables_score[table] = sum([(word in text) for word in self.words])\n",
    "            print(tables_score)\n",
    "            return [kv[0] for kv in tables_score.items() if kv[1] == max(tables_score.values())]\n",
    "\n",
    "    #ChatGPTに質問を投げて、表から回答を抽出する\n",
    "    def answer(self, ):\n",
    "        tables = self.answer_table()\n",
    "        if len(tables) == 1:\n",
    "            return tables[0]\n",
    "        \n",
    "        else:\n",
    "            table_text = ''\n",
    "            for i, table in enumerate(tables):\n",
    "                if os.path.exists(f'../data/{self.doc_id}/{table}.pickle'):                   \n",
    "                    with open(f'../data/{self.doc_id}/{table}.pickle', 'r',) as f:\n",
    "                        table_text +=f\"\"\"## Table{i}\\n{f.read()}\\n\"\"\"\n",
    "                        f.close()\n",
    "        \n",
    "            prompt =f\"質問に回答が記載されている表の番号を教えてください。\\n##question\\n{self.question}\\n{table_text}\\n##Output\\nTable_num:<Table_num>\"\n",
    "            \n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}, \n",
    "                ]\n",
    "            response = client.chat.completions.create(\n",
    "            model= engine,\n",
    "            messages = messages,\n",
    "            temperature=0,\n",
    "            max_tokens=2048,\n",
    "            top_p=0.1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None\n",
    "            )\n",
    "            return response.choices[0].message.content        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['研究開発費', '販売費及び一般管理費']\n",
      "S100RH38-0105010-tab99\n",
      "{'S100RH38-0105010-tab141': 0, 'S100RH38-0105010-tab163': 0, 'S100RH38-0105010-tab121': 0, 'S100RH38-0105010-tab146': 0, 'S100RH38-0105020-tab206': 0, 'S100RH38-0105010-tab165': 0, 'S100RH38-0105010-tab193': 0, 'S100RH38-0105020-tab224': 0, 'S100RH38-0105010-tab152': 0, 'S100RH38-0105010-tab175': 0, 'S100RH38-0105010-tab91': 0, 'S100RH38-0105010-tab142': 0, 'S100RH38-0105010-tab94': 0, 'S100RH38-0105010-tab137': 0, 'S100RH38-0105010-tab179': 0, 'S100RH38-0105010-tab122': 0, 'S100RH38-0105010-tab132': 0, 'S100RH38-0105020-tab220': 0, 'S100RH38-0105010-tab148': 0, 'S100RH38-0105020-tab223': 0, 'S100RH38-0105010-tab89': 0, 'S100RH38-0105010-tab149': 0, 'S100RH38-0105010-tab183': 0, 'S100RH38-0105010-tab176': 0, 'S100RH38-0105010-tab138': 0, 'S100RH38-0105020-tab214': 0, 'S100RH38-0105010-tab128': 0, 'S100RH38-0105010-tab171': 0, 'S100RH38-0105010-tab101': 0, 'S100RH38-0105010-tab102': 0, 'S100RH38-0105010-tab133': 0, 'S100RH38-0105010-tab156': 0, 'S100RH38-0105010-tab117': 0, 'S100RH38-0105010-tab119': 0, 'S100RH38-0105010-tab168': 0, 'S100RH38-0105010-tab124': 0, 'S100RH38-0105010-tab116': 0, 'S100RH38-0105010-tab147': 0, 'S100RH38-0105020-tab210': 0, 'S100RH38-0105020-tab221': 0, 'S100RH38-0105010-tab106': 0, 'S100RH38-0105010-tab113': 0, 'S100RH38-0105020-tab225': 0, 'S100RH38-0105010-tab104': 0, 'S100RH38-0105010-tab135': 0, 'S100RH38-0105010-tab151': 0, 'S100RH38-0105010-tab153': 0, 'S100RH38-0105010-tab157': 0, 'S100RH38-0105010-tab181': 0, 'S100RH38-0105010-tab194': 0, 'S100RH38-0105010-tab160': 0, 'S100RH38-0105010-tab136': 0, 'S100RH38-0105010-tab167': 0, 'S100RH38-0105010-tab143': 0, 'S100RH38-0105010-tab188': 0, 'S100RH38-0105010-tab93': 0, 'S100RH38-0105010-tab170': 0, 'S100RH38-0105010-tab159': 0, 'S100RH38-0105010-tab162': 0, 'S100RH38-0105010-tab134': 0, 'S100RH38-0105010-tab115': 0, 'S100RH38-0105010-tab180': 0, 'S100RH38-0105020-tab215': 0, 'S100RH38-0105010-tab145': 0, 'S100RH38-0105010-tab140': 0, 'S100RH38-0105010-tab184': 0, 'S100RH38-0105010-tab154': 0, 'S100RH38-0105010-tab155': 0, 'S100RH38-0105010-tab126': 0, 'S100RH38-0105020-tab208': 0, 'S100RH38-0105020-tab219': 0, 'S100RH38-0105010-tab131': 0, 'S100RH38-0105010-tab173': 0, 'S100RH38-0105010-tab108': 0, 'S100RH38-0105010-tab125': 0, 'S100RH38-0105010-tab189': 0, 'S100RH38-0105020-tab205': 0, 'S100RH38-0105010-tab123': 0, 'S100RH38-0105010-tab185': 0, 'S100RH38-0105010-tab191': 0, 'S100RH38-0105010-tab107': 0, 'S100RH38-0105010-tab90': 0, 'S100RH38-0105010-tab92': 0, 'S100RH38-0105010-tab139': 0, 'S100RH38-0105010-tab130': 0, 'S100RH38-0105010-tab158': 0, 'S100RH38-0105010-tab177': 0, 'S100RH38-0105010-tab95': 0, 'S100RH38-0105010-tab144': 0, 'S100RH38-0105010-tab186': 0, 'S100RH38-0105010-tab118': 0, 'S100RH38-0105010-tab182': 0, 'S100RH38-0105010-tab111': 0, 'S100RH38-0105020-tab213': 0, 'S100RH38-0105020-tab207': 0, 'S100RH38-0105020-tab216': 0, 'S100RH38-0105010-tab169': 0, 'S100RH38-0105010-tab174': 0, 'S100RH38-0105010-tab97': 0, 'S100RH38-0105020-tab209': 0, 'S100RH38-0105010-tab103': 0, 'S100RH38-0105010-tab166': 0, 'S100RH38-0105010-tab127': 0, 'S100RH38-0105010-tab150': 0, 'S100RH38-0105020-tab211': 0, 'S100RH38-0105010-tab190': 0, 'S100RH38-0105020-tab217': 1, 'S100RH38-0105010-tab112': 0, 'S100RH38-0105010-tab99': 1, 'S100RH38-0105010-tab96': 0, 'S100RH38-0105020-tab212': 0, 'S100RH38-0105010-tab192': 0, 'S100RH38-0105010-tab120': 0, 'S100RH38-0105010-tab164': 0, 'S100RH38-0105010-tab98': 2, 'S100RH38-0105010-tab105': 0, 'S100RH38-0105020-tab218': 0, 'S100RH38-0105010-tab109': 0, 'S100RH38-0105010-tab172': 0, 'S100RH38-0105010-tab161': 1, 'S100RH38-0105010-tab110': 0, 'S100RH38-0105010-tab187': 0, 'S100RH38-0105010-tab129': 0, 'S100RH38-0105010-tab178': 0, 'S100RH38-0105020-tab222': 0, 'S100RH38-0105010-tab114': 0, 'S100RH38-0105010-tab100': 0}\n",
      "['S100RH38-0105010-tab98']\n"
     ]
    }
   ],
   "source": [
    "df_train = data.query('flag in [\"train\", \"valid\"]')\n",
    "question, words, doc_id, table_id = data.query('question_number ==\"question_test34\"')[['question',\"words\",\"doc_id\", 'table_id']].values[0]\n",
    "words = json.loads(words.replace(\"'\", '\"'))\n",
    "print(words)\n",
    "print(table_id)\n",
    "print(search_table(words, doc_id, df_train, question).answer_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "df_train = data.query('flag in [\"train\", \"valid\"]')\n",
    "answer_sheet = {}\n",
    "for index, row in data.query('flag ==\"test\"')[:20].iterrows():\n",
    "    words, doc_id, table_id = row[[\"words\",\"doc_id\", 'table_id']]\n",
    "    words = json.loads(words.replace(\"'\", '\"'))\n",
    "    answer = search_table(words, doc_id, df_train).answer_table()\n",
    "    answer_sheet[row['question_number']] = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_test33, TRUE:S100RH38-0105010-tab99, PRED:['S100RH38-0105010-tab98']\n",
      "question_test34, TRUE:S100RH38-0105010-tab99, PRED:['S100RH38-0105010-tab98']\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for key in answer_sheet.keys():\n",
    "    total += 1\n",
    "    if gold_sd[key] in answer_sheet[key]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        print(f'{key}, TRUE:{gold_sd[key]}, PRED:{answer_sheet[key]}')\n",
    "        pass\n",
    "        \n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json.dump(answer_sheet,open('../test/answersheet.json', 'w') ,indent=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
